{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_5 Automatic Differentiation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNJy6P4Kf+VMfPCDBax34ZW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mario-U-KY/DataScience/blob/main/2_5_Automatic_Differentiation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> 2.5 Automatic Differentiation"
      ],
      "metadata": {
        "id": "zXKqrQgVg_7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "미분은 딥러닝에서 중요한 스탭이나, 복잡한 모델에서 수동적으로 작업하는 것은 굉장히 어렵고 에러를 유발한다. 이에 딥러닝 프레임워크를 이용해서 자동적으로 미분값을 계산하는 방법을 사용한다. 이는 시스템으로 하여금 각 파라미터에 대한 편미분을 채워가며 computational graph를 추적하는 backpropagate를 의미한다"
      ],
      "metadata": {
        "id": "5uSR8KlChbqe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> 2.5.1 A Simple Example"
      ],
      "metadata": {
        "id": "CpmyygsahIKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "jkjOthBWhZPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#위의 식을 x에 관하여 미분하기\n",
        "import torch\n",
        "x = torch.arange(4.0)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP0iFyyNh_d1",
        "outputId": "fdd9f3cf-deda-4513-e02f-1f4186f59724"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "계속 파라미터를 업데이트 할 것이기 때문에 항상 메모리를 할당해서는 안된다. 따라서 메모리를 보관할 storage가 필요."
      ],
      "metadata": {
        "id": "_63zwMuTiEWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.requires_grad_(True) # tensor에서 이뤄진 모든 연산들을 추적한다.\n",
        "#계산이 완료된 후 .backward()로 모든 변화도(gradient)를 자동 계산 가능하며\n",
        "#이 텐서의 변화도는 .grad의 속성에 누적된다.\n",
        "#기록 추적 중단을 위해서는 .detach()를 호출하면 멈출 수 있음\n",
        "x.grad"
      ],
      "metadata": {
        "id": "yV6EHjTNiTix"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = 2*torch.dot(x,x)\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DH0sAMYZiXQV",
        "outputId": "9e13926a-c54e-4768-b4d6-2d7f46bd5fbe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(28., grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "X의 길이 4이므로, x와x의 내적이 수행되었고, y에 scalar output이 할당되었다. 이제 y의 gradient를 backpropagation으로 계산할 수 있음"
      ],
      "metadata": {
        "id": "qf83bPueieWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()\n",
        "x.grad #결과는 x=0,1,2,3에서 y의 미분값이 나온다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHWo47yWjUfM",
        "outputId": "5f4e98a9-1748-495b-a79b-5f638ee3392c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.,  4.,  8., 12.])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad == 4*x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huhbAMv7jVnf",
        "outputId": "57a4b730-6916-4dc5-b953-794ab8bf1403"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch accumulates the gradient in default, we need to clear the previous\n",
        "# values\n",
        "x.grad.zero_()\n",
        "y = x.sum() #y = x1+x2+x3+... 가된다.\n",
        "y.backward() #y 함수에 대한 x1,x2,x3,x4의 편미분값이 반환됨(모두 1)\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sqys-YxNjwmg",
        "outputId": "da5b7733-00fd-4fe4-cca2-94f9f046ab0b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>2.5.2 Backward for Non-Scalar Variables"
      ],
      "metadata": {
        "id": "-0l7DUIZj8BG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "y가 스칼라 형태가 아닌경우, 벡터 y를 벡터 x로 미분한 값은 행렬의 형태로 나온다. 더 고차원의 y,x에 대해 그 미분 결과는 high-order tensor가 도출된다 "
      ],
      "metadata": {
        "id": "fb9AXl5mnJzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "하지만, deep learning에서 벡터에 대한 backward를 진행할 때, 우리는 각 베치(batch)당 loss 함수의 미분값을 계산하려는 의도를 가지고 있다. 따라서, 우리는 미분행렬 자체를 구하는 것이 아니라, batch당, 개별적으로 구해진 편미분값의 합을 원한다."
      ],
      "metadata": {
        "id": "H3LwR0aKndaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoking `backward` on a non-scalar requires passing in a `gradient` argument\n",
        "# which specifies the gradient of the differentiated function w.r.t `self`.\n",
        "# In our case, we simply want to sum the partial derivatives, so passing\n",
        "# in a gradient of ones is appropriate\n",
        "x.grad.zero_() #x의 gradient 초기화\n",
        "y = x * x\n",
        "# y.backward(torch.ones(len(x))) equivalent to the below\n",
        "y.sum().backward() #grad값은 scalar 값에서만 구해질 수 있다. -> summation\n",
        "y, x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAHCszyKn3Ke",
        "outputId": "503e556a-a4de-48de-e454-6cb58f290c64"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>), tensor([0., 2., 4., 6.]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> 2.5.3 Detaching Computation"
      ],
      "metadata": {
        "id": "dZ9niN35o855"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "우리는 computational graph 바깥에서 calculation을 기록하길 원하는 경우도 있다. y값이 x의 함수에 의해 계산되어졌고 연속적으로 z 가, y와 x에 의해 계산되어졌다고 가정하면, x에대한 z의 gradient 값을 구하기 위해 y를 상수로 여겨야 하며, y가 계산 된 이후 \"X가 한 역할만\" 구해내야 한다.\n",
        "\n",
        "<br>\n",
        "이에 y를 떼서 새로운 변수인 u(값은 같으나 정보를 기록하지 않는)를 대신 집어넣는 방법으로 gradient가 u를 거쳐서 x로 가지 않게끔 방지한다. z = u*x에 u를 상수로 여겨 z = x*x*x 대신에 사용한다"
      ],
      "metadata": {
        "id": "NlCh551Po-da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_()#gradient 초기화\n",
        "y = x*x\n",
        "u = y.detach()\n",
        "z = u*x\n",
        "z.sum().backward()\n",
        "x.grad == u #u가 상수로 쓰였으므로, ux를 x로 미분한 것은 u가 됨."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sBzJBs3q_PR",
        "outputId": "909a4642-72e8-47d1-ee2d-a276e3e348ab"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "y에대한 연산이 기록되었으므로, y=x*x에 대한 backpropagation을 진행할 수 있다."
      ],
      "metadata": {
        "id": "yyFQFfGvsMUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_()\n",
        "y.sum().backward()\n",
        "x.grad==2*x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3MWzqHLsUx4",
        "outputId": "7aee7953-a83e-45a7-c942-fe5fd1215ea7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> 2.5.4 Computing the Gradient of Python Control Flow"
      ],
      "metadata": {
        "id": "bq5Tgc8jsYx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "automatic differentiation을 사용해 얻는 이득은 파이썬의 복잡한 control flow(조건문, 반복문, 함수 등)을 거쳐서도 결과로 지정된 변수를 활용하면 gradient를 구할 수 있다는 것이다."
      ],
      "metadata": {
        "id": "jnDoErjOs52b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#아주 복잡하고 할당이 다양한 함수이나 a값에 따라 결국 k*a의 형태의 식을 반환\n",
        "def f(a):\n",
        "  b = a*2\n",
        "  while b.norm() < 1000:\n",
        "    b = b*2\n",
        "  if b.sum() > 0:\n",
        "    c = b\n",
        "  else:\n",
        "    c = 100*b\n",
        "  return c\n"
      ],
      "metadata": {
        "id": "Ux2D0kdutS1j"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn(size=(), requires_grad=True)\n",
        "d = f(a)\n",
        "d.backward()\n",
        "a.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJYiZuYztbsy",
        "outputId": "06c55dd2-cb5f-44b3-e844-f80f59c40fa5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1638400.)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.grad == d/a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5snyJ3kut3IR",
        "outputId": "33b70452-c286-4a6a-d0ae-81004a22e1dd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "즉 d의 a에대한 gradient값이, d/a의 값을 수행한 결과와 같다. 즉 f(x)를 거쳐서 나온 식이 상수배의 형태로 표시된다는 것을 증명할 수 있다."
      ],
      "metadata": {
        "id": "-SGN8z8quN-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> 2.5.5 summary </h2>\n",
        "정리하자면, python의 autograd를 사용해서 내가 편미분 하고싶은 파라미터(a)에 'requires_grad = True'로 트랙킹을 활성화 시키고, 최종 result에서 .backward()함수를 이용해 a.grad로 result의 변수 a에 대한 편미분 값을 확인할 수 있다. <br> 파라미터가 한개인 식 ex) y =  x*x의 경우 y의 미분값인 2x라는 도함수를 구하는 것과 같아지고 다변량의 경우에는 attach된 파라미터에 대한 partial derivative 값이 도출된다는 것을 유념하자.\n",
        "<br>\n",
        "파이썬의 이러한 툴은 메모리 누수를 방지하고, 복잡한 연산과정에서 생기는 오류를 방지하는 등 다양한 장점이 존재한다."
      ],
      "metadata": {
        "id": "W_wgCwcyuH9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Zaolr_47vGL1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}